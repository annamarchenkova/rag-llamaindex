{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14304a78",
   "metadata": {},
   "source": [
    "# RAG implementation\n",
    "### STEPS\n",
    "1. **CLEANING THE DATA**\n",
    "- Removing `unnecessary docs`, updating docs, removing conflicting information\n",
    "2. **READING THE DATA** \n",
    "- Python, OCR, etc\n",
    "3. **CHUNKING** \n",
    "- `small chunks`: more relevant info, but no context; smaller prompts\n",
    "- `large chunks`: more context; costly prompts\n",
    "4. **EMBEDDING** >> **VECTOR DB**\n",
    "- `different models` for embeddings\n",
    "- different VDB\n",
    "- adding `metadata`, `chapter names`, other info to improve retrieval\n",
    "- storing hypothetical questions that each chunk responds to, and not the chunk itself\n",
    "5. **FINE-TUNING** \n",
    "- To remove `hallucinations` and improve performance. \n",
    "- `on the data`: prompt + chunks >> correct answer (including \"I don't know\") \n",
    "- just reating the database to improve the performance (much less effective)\n",
    "6. **RETRIEVAL** \n",
    "- Retreaving more chunks >> `reranking` with anohter model\n",
    "- Cosine similarity + `other methods`: keywords\n",
    "- `Rewriting` user's question to be more inline with the documents style \n",
    "- `Cash FAQs` and the answers. Check first if the Q is similar to FAQ: \\ \n",
    "        1). Q FAQ similarity > thresh => return Answer \\\n",
    "        2). thresh2 < Q FAQ similarity < thresh => add Answer to the prompt as another chunk \\\n",
    "        3). otherwise => use normal strategy with chunks\n",
    "7. **EVALUATION**\n",
    "- guardrails ai  \n",
    "- phoenix rag evaluation\n",
    "8. **OPTIMIZATION**\n",
    "- proprietary model (trained, fine-tuned)\n",
    "- cheaper models for Q rewriting and creation of evaluation data\n",
    "- quantization of the model (before putting in prodd), LORA (low rank adoptation of the weights)\n",
    "- \n",
    "9. **PRODUCTION**\n",
    "- telegram bot + whisper \n",
    "- app in selenium "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c7953",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1045e8-43a6-4c36-be6a-89743e6055c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install chromadb==0.4.18\n",
    "! pip install ipython==8.18.1\n",
    "! pip install llama_index==0.9.13\n",
    "\n",
    "# also necessary for used llamaindex functionalities\n",
    "! pip install pypdf==4.0.1\n",
    "! pip install spacy==3.7.2\n",
    "! pip install guardrails-ai==0.3.2\n",
    "! pip install openpyxl==3.1.2\n",
    "! pip install openai-whisper==20231117 python-telegram-bot==20.7\n",
    "! pip install pydub==0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12182ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    ServiceContext, \n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.evaluation import (\n",
    "    DatasetGenerator, \n",
    "    RelevancyEvaluator, \n",
    "    ResponseEvaluator, \n",
    "    FaithfulnessEvaluator, \n",
    "    QueryResponseEvaluator,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.schema import TextNode\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.postprocessor import TimeWeightedPostprocessor, SimilarityPostprocessor\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.output_parsers import GuardrailsOutputParser\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.response_synthesizers import (\n",
    "    get_response_synthesizer,\n",
    "    BaseSynthesizer,\n",
    "    TreeSummarize,\n",
    ")\n",
    "from embedding_manager import Embeddings\n",
    "from llm_manager import LLMMain \n",
    "\n",
    "import guardrails as gd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from guardrails.validators import QARelevanceLLMEval, TwoWords, ToxicLanguage, ProvenanceV1, SimilarToDocument\n",
    "\n",
    "from project_dirs import PROJECT_DIR, DATA_DIR, OUTPUT_DIR\n",
    "from utils import (\n",
    "    load_config, \n",
    "    list_all_filepaths_for_list_of_extentions, \n",
    "    list_all_filepaths, \n",
    "    get_eval_df\n",
    ")\n",
    "from data_loader import DataLoader, DatabaseManager\n",
    "from query_engine import RAGStringQueryEngine\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# To avoid RuntimeError: asyncio.run() cannot be called from a running event loop when running in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5d8ae",
   "metadata": {},
   "source": [
    "## Parameters, Env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0289c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnf = load_config(cnf_dir=PROJECT_DIR, cnf_name=\"config.yml\")\n",
    "\n",
    "os.environ['OPENAI_API_TYPE'] = cnf['openai_api_type']\n",
    "os.environ[\"OPENAI_API_VERSION\"] = cnf['openai_api_type']\n",
    "os.environ['OPENAI_API_KEY'] = open(os.path.join(PROJECT_DIR, \"keys\", cnf['openai_key_file']), \"r\").read().strip(\"\\n\")\n",
    "\n",
    "# os.environ['AZURE_EMBEDDING_MODEL'] = cnf['azure_embeddign_model']\n",
    "# os.environ['AZURE_LLM_MODEL'] = cnf['azure_llm_model']\n",
    "# os.environ['AZURE_DEPLOYMENT_NAME'] = cnf['azure_deployment_name']\n",
    "# os.environ['AZURE_EMBEDDING_DEPLOYMENT_NAME'] = cnf['azure_deployment_name_embeddigns']\n",
    "# os.environ['AZURE_OPENAI_API_KEY'] = open(os.path.join(PROJECT_DIR, \"keys\", cnf['azure_openai_key_file']), \"r\").read().strip(\"\\n\")\n",
    "# os.environ['AZURE_OPENAI_ENDPOINT'] = cnf['azure_openai_api_endpoint']\n",
    "# os.environ['AZURE_API_VERSION'] = cnf['azure_openai_api_version']\n",
    "# os.environ['OPENAI_API_BASE'] = cnf['azure_openai_api_endpoint']\n",
    "\n",
    "db_path = os.path.join(PROJECT_DIR, 'vector_store')\n",
    "db_collection_name = cnf['db_collection_name']\n",
    "embedding_mode = cnf['embedding_mode']\n",
    "llm_mode = cnf['llm_mode']\n",
    "llm_model_path = cnf['llm_model_path']\n",
    "embedding_mode = cnf['embedding_mode']\n",
    "local_embeddings_model_name = cnf['local_embeddings_model_name']\n",
    "chunk_size = cnf['chunk_size']\n",
    "chunk_overlap = cnf['chunk_overlap']\n",
    "data_path = os.path.join(DATA_DIR, 'main_data')\n",
    "\n",
    "indexid = f'{db_collection_name}_index'\n",
    "index_path = os.path.join(PROJECT_DIR, 'vs_index')\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"Le informazioni contestuali sono riportate di seguito.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Date le informazioni di contesto e non la conoscenza pregressa, \"\n",
    "    \"rispondi alla query.\\n\"\n",
    "    \"Se l'informazione non è nel contesto rispondere 'Informazione non trovata'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db12769-f05a-4d92-9f91-3db1a98e007d",
   "metadata": {},
   "source": [
    "## LLM, Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6749ae-0fd2-4b66-b095-ce5c1dc38395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main LLM\n",
    "main_llm = LLMMain(llm_mode, llm_model_path)\n",
    "llm = main_llm.llm\n",
    "\n",
    "# Embedding Model \n",
    "embedding = Embeddings(embedding_mode, local_model_name=None)\n",
    "embedding_model = embedding.embedding_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b317af-8a1b-4535-a01c-94225a6c6aea",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee63e1-14ef-4e13-a89a-f79b3711ab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chroma Database \n",
    "db_manager = DatabaseManager(db_path=db_path, collection_name=db_collection_name)\n",
    "db_collection = db_manager.get_db()\n",
    "\n",
    "# Vector Store and Index for main data\n",
    "vector_store = ChromaVectorStore(chroma_collection=db_collection)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=llm)\n",
    "\n",
    "# Retreive if exists otherwise create new\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store,\n",
    "                                                  persist_dir=index_path)\n",
    "    index = load_index_from_storage(\n",
    "        service_context=service_context, # to get correctly the models too\n",
    "        storage_context=storage_context, \n",
    "        index_id=indexid, \n",
    "        llm=None\n",
    "    )\n",
    "    logger.info(f\"Loaded {indexid} from local path {index_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR:{e}\")\n",
    "    logger.info(\"Creating the vector index\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    # service_context = ServiceContext.from_defaults(embed_model=embedding_model, llm=llm)\n",
    "    \n",
    "    # Load and chunk data files\n",
    "    file_paths = list_all_filepaths_for_list_of_extentions(common_dir=data_path)\n",
    "    data_loader = DataLoader(file_paths=file_paths)\n",
    "    data = data_loader.read_data()\n",
    "    chunks = data_loader.chunk_data(\n",
    "        data, \n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    excluded_embed_metadata_keys=[\n",
    "        'file_type', \n",
    "        'file_size', \n",
    "        'creation_date', \n",
    "        'last_modified_date', \n",
    "        'last_accessed_date'\n",
    "    ]\n",
    "    for chunk in chunks:\n",
    "        chunk.excluded_embed_metadata_keys = excluded_embed_metadata_keys\n",
    "    index =  VectorStoreIndex(\n",
    "        chunks, storage_context=storage_context, service_context=service_context\n",
    "    )\n",
    "    index.set_index_id(indexid)\n",
    "    index.storage_context.persist(persist_dir=index_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367ce5b-f444-45cb-a269-17756285d774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vector Store and Index for Excel QUESTIONS\n",
    "index_id_faq = 'faq_indexid'\n",
    "db_collection_faq_name = \"faq\"\n",
    "\n",
    "# Chroma Database \n",
    "db_manager_faq = DatabaseManager(db_path=db_path, collection_name=db_collection_faq_name)\n",
    "db_collection_faq = db_manager_faq.get_db()\n",
    "\n",
    "vector_store_faq = ChromaVectorStore(chroma_collection=db_collection_faq)\n",
    "service_context_faq = ServiceContext.from_defaults(embed_model=embedding_model, llm=llm)\n",
    "\n",
    "# Retreive if exists otherwise create new\n",
    "try:\n",
    "    storage_context_faq = StorageContext.from_defaults(vector_store=vector_store_faq,\n",
    "                                                  persist_dir=index_path)\n",
    "    index_faq = load_index_from_storage(\n",
    "        service_context=service_context_faq, # to get correctly the models too\n",
    "        storage_context=storage_context_faq, \n",
    "        index_id=index_id_faq, \n",
    "        llm=None\n",
    "    )\n",
    "    logger.info(f\"Loaded {indexid} from local path {index_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR:{e}\")\n",
    "    logger.info(\"Creating the vector index\")\n",
    "    storage_context_faq = StorageContext.from_defaults(vector_store=vector_store_faq)\n",
    "    service_context_faq = ServiceContext.from_defaults(embed_model=embedding_model, llm=llm)\n",
    "    \n",
    "    # Load and chunk data files\n",
    "    excel_files = list_all_filepaths(common_dir=DATA_DIR, folder='',extension='xlsx')\n",
    "    faq_df = pd.read_excel(excel_files[0])\n",
    "    excel_questions = faq_df.Domanda.values\n",
    "    excel_nodes = [TextNode(text=i) for i in excel_questions]\n",
    "\n",
    "    index_faq =  VectorStoreIndex(\n",
    "            excel_nodes, storage_context=storage_context_faq, service_context=service_context_faq\n",
    "        )\n",
    "    index_faq.set_index_id(index_id_faq)\n",
    "    index_faq.storage_context.persist(persist_dir=index_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a4e47-9ccc-4da0-88b3-4c56546d47c5",
   "metadata": {},
   "source": [
    "## Definizione di guardrails, query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5335e-a989-4990-8e5b-eae882bc86de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query engine\n",
    "####################################################################################\n",
    "# synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "similarity_postprocessor = SimilarityPostprocessor(similarity_cutoff=cnf['similarity_cutoff']) \n",
    "rerank_postprocessor = TimeWeightedPostprocessor(time_decay=0.5, time_access_refresh=False, top_k=2)\n",
    "# from llama_index.postprocessor import RankGPTRerank\n",
    "# gpt_preprocessor = RankGPTRerank(top_n=2, llm=llm)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=4)\n",
    "\n",
    "query_engine = RAGStringQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=synthesizer,\n",
    "    llm=llm,\n",
    "    qa_prompt=qa_prompt,\n",
    "    # postprocessor=similarity_postprocessor,\n",
    "    postprocessors=[similarity_postprocessor, rerank_postprocessor],\n",
    ")\n",
    "\n",
    "# configure FAQ retriever\n",
    "####################################################################################\n",
    "retriever_faq = VectorIndexRetriever(\n",
    "    index=index_faq,\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "faq_similarity_threshold = cnf['faq_similarity_threshold']\n",
    "\n",
    "# Guardrails\n",
    "####################################################################################\n",
    "# QARelevanceLLMEval \n",
    "qa_relevance_guard = gd.Guard.from_string(\n",
    "    validators=[\n",
    "        QARelevanceLLMEval(on_fail=\"fix\", llm_callable=\"gpt-3.5-turbo\"),\n",
    "    ],\n",
    "    description=\"\",\n",
    ")\n",
    "\n",
    "# ProvenanceV1\n",
    "def query_function(text: str, k: int, sources) -> List[str]:\n",
    "    # sources in ascending order\n",
    "    return sources[::-1][:k]\n",
    "\n",
    "provenance_v1_guard = gd.Guard.from_string(validators=[\n",
    "    ProvenanceV1(llm_callable=\"gpt-3.5-turbo\", on_fail=\"fix\", validation_method=\"full\",) #validation_method=\"sentence\"\n",
    "])\n",
    "\n",
    "def reformulate_question(llm, question):\n",
    "    prompt = f\"Riscrivere la domanda per rimuovere tutte le informazioni irrilevanti e lasciare solo domande chiare e concise. Domanda:{question}\" \n",
    "    return str(llm.complete(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643864ae-5734-4b63-af52-808f6d582b89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Telegram bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f5bf0-148a-4338-b7f6-1bd1e32cd2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INSTALL FFMPEG FIRST!!\n",
    "# sudo apt install ffmpeg \n",
    "# OR / AND\n",
    "# ! conda install --y ffmpeg\n",
    "\n",
    "from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackContext\n",
    "from telegram import Update\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "from project_dirs import DATA_DIR\n",
    "import whisper\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "from pydub import AudioSegment\n",
    "AudioSegment.converter = \"/opt/conda/bin/ffmpeg\"\n",
    "AudioSegment.ffmpeg = \"/opt/conda/bin/ffmpeg\"\n",
    "AudioSegment.ffprobe = \"/opt/conda/bin/ffprobe\"\n",
    "\n",
    "from utils import append_to_path\n",
    "for path in [\n",
    "    \"/opt/conda/bin/ffmpeg\",\n",
    "    \"/opt/conda/bin/ffprobe\"\n",
    "]:\n",
    "    append_to_path(path)\n",
    "\n",
    "from typing import Final\n",
    "TOKEN: Final = open(os.path.join(PROJECT_DIR, \"keys\", cnf['tg_token_file']), \"r\").read().strip(\"\\n\")\n",
    "# BOT_USERNAME: Final = '@qa_rag_bot'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f211ee-e387-41ab-aac2-bc981bb918da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMMANDS\n",
    "async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\n",
    "        \"\"\"Ciao! Sono un bot che risponde alle domande sul documento \n",
    "        'BANDO CONneSSi CONtributi per lo Sviluppo di Strategie digitali \n",
    "        per i mercati globali. Anno 2024. Fammi una domanda.\"\"\")\n",
    "    \n",
    "async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\n",
    "        \"\"\"Scrivi una domanda sul documento\n",
    "        'BANDO CONneSSi CONtributi per lo Sviluppo di Strategie digitali \n",
    "        per i mercati globali. Anno 2024. \"\"\")\n",
    "    \n",
    "async def custom_command(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\n",
    "        \"\"\".\"\"\")\n",
    "\n",
    "# RESPONSES\n",
    "def process_question(\n",
    "    question: str,\n",
    "    retriever_faq, \n",
    "    faq_similarity_threshold,\n",
    "    faq_df,\n",
    "    query_engine,\n",
    "    qa_relevance_guard,\n",
    "    provenance_v1_guard,\n",
    "    \n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Main processing function\n",
    "    \"\"\"\n",
    "    # To use a better formulated questions\n",
    "    question = reformulate_question(llm=llm, question=question)\n",
    "    \n",
    "    # Check similarity to sample questions (FAQ)\n",
    "    # Use FAQ answer if the user question is similar (faq_similarity_threshold: 0.9)\n",
    "    ######################\n",
    "    most_silmilar_node = retriever_faq.retrieve(question)[0]\n",
    "    if most_silmilar_node.score > faq_similarity_threshold:\n",
    "        node_text = most_silmilar_node.text\n",
    "        faq_answer = faq_df.loc[faq_df.Domanda==node_text, \"Risposta\"].values[0]\n",
    "        answer = f\"Rispondo alla domanda simile. {node_text}: {faq_answer}\"\n",
    "        \n",
    "        logging.info(\"Returning stored answer to the question %s\", node_text)\n",
    "    else:\n",
    "        answer = 'Informazione non trovata'\n",
    "        # Query with reranking\n",
    "        ######################\n",
    "        raw_answer, source_nodes = query_engine.query(question)\n",
    "        if len(source_nodes) != 0:\n",
    "            # Apply guardrails ai\n",
    "            ######################\n",
    "            raw_llm_output, validated_output, *rest = qa_relevance_guard.parse(\n",
    "            llm_output=raw_answer, metadata={'question':question}\n",
    "            )\n",
    "            if validated_output:\n",
    "                raw_llm_output, validated_output, *rest = provenance_v1_guard.parse(\n",
    "                    llm_output=raw_answer, \n",
    "                    metadata={'query_function':partial(query_function, sources=[i.text for i in source_nodes])}\n",
    "                )\n",
    "                if validated_output:\n",
    "                    answer = validated_output\n",
    "    return answer\n",
    "\n",
    "async def handle_message(\n",
    "    update: Update, \n",
    "    context: ContextTypes.DEFAULT_TYPE,\n",
    "):\n",
    "    message_type: str = update.message.chat.type\n",
    "    text: str = update.message.text\n",
    "    \n",
    "    # logging.info(\"User %s in %s: '%s'\", update.message.chat.id, message_type, text)\n",
    "    print(\"User {update.message.chat.id} in {message_type}: {text}\")\n",
    "    \n",
    "    response: str = process_question(\n",
    "                        text,\n",
    "                        retriever_faq, \n",
    "                        faq_similarity_threshold,\n",
    "                        faq_df,\n",
    "                        query_engine,\n",
    "                        qa_relevance_guard,\n",
    "                        provenance_v1_guard,\n",
    "                    )\n",
    "    print('Bot:', response)\n",
    "    await update.message.reply_text(response)\n",
    "\n",
    "def convert_ogg_to_mp3(ogg_filepath, file_id):\n",
    "    mp3_filepath = os.path.join(DATA_DIR, f\"{file_id}.mp3\")\n",
    "    audio = AudioSegment.from_file(ogg_filepath, format=\"ogg\")\n",
    "    audio.export(mp3_filepath, format=\"mp3\")\n",
    "    return mp3_filepath\n",
    "\n",
    "def convert_speech_to_text(audio_filepath, model):\n",
    "    data = model.transcribe(audio_filepath)\n",
    "    return data[\"text\"]\n",
    "    \n",
    "async def handle_voice_message(\n",
    "    update: Update, \n",
    "    context: CallbackContext,\n",
    "):   \n",
    "    message_type = update.message.chat.type\n",
    "    file_id = update.message.voice.file_id\n",
    "    # print(f\"file_id {file_id}\")\n",
    "    new_file = await context.bot.get_file(file_id)\n",
    "    # print(f\"new_file {new_file}\")  \n",
    "    await new_file.download_to_drive(f\"{file_id}.ogg\")\n",
    "\n",
    "    mp3_filepath = convert_ogg_to_mp3(f\"{file_id}.ogg\", file_id)\n",
    "    extracted_text = convert_speech_to_text(mp3_filepath, whisper_model)\n",
    "\n",
    "    response: str = process_question(\n",
    "                    extracted_text,\n",
    "                    retriever_faq, \n",
    "                    faq_similarity_threshold,\n",
    "                    faq_df,\n",
    "                    query_engine,\n",
    "                    qa_relevance_guard,\n",
    "                    provenance_v1_guard,\n",
    "                )\n",
    "    print('Bot:', response)\n",
    "    await update.message.reply_text(response)\n",
    "    os.remove(f\"{file_id}.ogg\")\n",
    "    os.remove(mp3_filepath)\n",
    "    \n",
    "# ERRORS    \n",
    "async def error(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    print(f\"Update {update} caused error {context.error}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef772d9b-54fa-4d37-9694-f4ec3b2235b9",
   "metadata": {},
   "source": [
    "### Run bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0fc5b-0729-4eff-9395-17b4b716766d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# whisper_model = whisper.load_model(\"base\")\n",
    "print('Starting bot')\n",
    "app = Application.builder().token(TOKEN).build()\n",
    "\n",
    "# Commands\n",
    "app.add_handler(CommandHandler('start', start_command))\n",
    "app.add_handler(CommandHandler('help', start_command))\n",
    "# app.add_handler(CommandHandler('custom', start_command))\n",
    "\n",
    "# Messages\n",
    "app.add_handler(MessageHandler(filters.TEXT, handle_message))\n",
    "app.add_handler(MessageHandler(filters.VOICE, handle_voice_message))\n",
    "\n",
    "# Errors\n",
    "app.add_error_handler(error)\n",
    "\n",
    "app.run_polling(poll_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a8f81-71f1-4169-9922-e142944012f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_rag_bot\n",
    "@qa_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60f094-250e-4dc7-bde2-c2393b7a2f44",
   "metadata": {},
   "source": [
    "## Evaluate RAG with ResponseEvaluator\n",
    "https://github.com/nguyenkien1402/llamaindex-practices/blob/main/evaluation-pipeline-rag/rag_evaluation_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59862e6e-1c54-47f7-a0fe-3a545b9767c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b4be3-3333-4fa8-a0f7-c2c73c4d3555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86fea2-0c14-4139-8394-ed71c763e589",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = chunks\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=2\n",
    ")\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8b134-42c0-4d59-959e-878b31891fed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever1 = index.as_retriever(similarity_top_k=2)\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results(\"Embedding Retriever\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b310aa-44b2-4235-a11c-9ced2f1b7220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2699f03-9fe0-449e-ab3c-6939a098e3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_results(\"OpenAI Embedding Retriever\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a67032-7ad7-48a1-ad7b-29fb9f7012d1",
   "metadata": {},
   "source": [
    "## Answers evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b34463-d575-4670-9cfe-6a92a3e00f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = faq_df.Domanda.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c7950-a1d7-41de-a80e-e39629147750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo\n",
    "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
    "\n",
    "# gpt-4\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e351289-8f29-429f-9cf1-4cfb74b476d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector_index35 = VectorStoreIndex(chunks, service_context = service_context_gpt35)\n",
    "# query_engine35 = vector_index35.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b7f61-8b35-4eb7-9245-5c2c07b487e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.response.schema import Response\n",
    "\n",
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "\n",
    "from llama_index.evaluation import RelevancyEvaluator\n",
    "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb2628-0f81-4907-945f-1b1b86a78934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "eval_query = queries[10]\n",
    "print(eval_query)\n",
    "\n",
    "raw_answer, source_nodes = query_engine.query(eval_query)\n",
    "answer = Response(response=raw_answer, source_nodes=source_nodes)\n",
    "\n",
    "eval_result = faithfulness_gpt4.evaluate_response(response=answer)\n",
    "print(f\"Faithfulness Evaluation: {eval_result.passing}\")\n",
    "\n",
    "eval_result = relevancy_gpt4.evaluate_response(\n",
    "    query=eval_query, response=answer\n",
    ")\n",
    "print(f\"Relevancy Evaluation: {eval_result.passing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbf770-58a3-496f-9db5-6afee9674ed0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch analysis\n",
    "faithfulness_results = []\n",
    "relevancy_results = []\n",
    "for eval_query in queries[:10]:\n",
    "    \n",
    "    # get answer, build Response\n",
    "    raw_answer, source_nodes = query_engine.query(eval_query)\n",
    "    answer = Response(response=raw_answer, source_nodes=source_nodes)\n",
    "    \n",
    "    f_result = faithfulness_gpt4.evaluate_response(response=answer)\n",
    "    r_result = relevancy_gpt4.evaluate_response(\n",
    "    query=eval_query, response=answer\n",
    ")\n",
    "    faithfulness_results.append(f_result)\n",
    "    relevancy_results.append(r_result)\n",
    "    \n",
    "faithfulness_score = sum(result.passing for result in faithfulness_results) / len(faithfulness_results)\n",
    "relevance_score = sum(result.passing for result in relevancy_results) / len(relevancy_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d749c6-6625-439b-adb0-cb5e0c5fc842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"faithfulness_score: {faithfulness_score}\")\n",
    "print(f\"relevance_score: {relevance_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9bdf8-a6e4-4f87-b748-6a1adcbfc8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bb9d4-c7c6-4d02-a182-fd7763e55f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854c6dc-764b-4db0-a150-989fad734654",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = faq_df.Domanda.values\n",
    "evaluator = ResponseEvaluator(service_context=service_context)\n",
    "\n",
    "eval_dfs = []\n",
    "for ix, question in enumerate(questions):\n",
    "    raw_answer, source_nodes = query_engine.query(question)\n",
    "    response = Response(response=raw_answer, source_nodes=source_nodes)\n",
    "    # response = query_engine.query(question)\n",
    "    logging.info(\"############  EVALUATING RESULT: %s  ############\", ix)\n",
    "    eval_result = evaluator.evaluate_response(response=response)\n",
    "    eval_dfs.append(get_eval_df(question, response, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa09871-4cfa-4a7c-bef6-aed2835085e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = eval_df[eval_df['Evaluation Result']=='YES'].shape[0]\n",
    "perc_correct = correct/eval_df.shape[0]*100\n",
    "print(f\"Correct: {perc_correct :.2f} %, {eval_df.shape[0]} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a11c0-dc3d-4719-9248-a0739e616669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f41c1-e129-4ebb-a44e-2483389b5957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eee760a3-e0ed-4114-8922-e4d2c2c38846",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5cdf44-09c5-45ce-92c7-931f7fa47d2b",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b80096-96c6-461a-9b5b-3aad0b7590a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = RAGStringQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=synthesizer,\n",
    "    llm=llm,\n",
    "    qa_prompt=qa_prompt,\n",
    "    # postprocessor=similarity_postprocessor,\n",
    "    postprocessors=[similarity_postprocessor, rerank_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d576ec-4fdb-4011-a992-0e93ffc535f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"dimmi per favore qual'è l'obiettivo del bando connessi?\"\n",
    "\n",
    "answer = 'Informazione non trovata'\n",
    "# Query with reranking\n",
    "######################\n",
    "raw_answer, source_nodes = query_engine.query(question)\n",
    "if len(source_nodes) != 0:\n",
    "    # Apply guardrails ai\n",
    "    ######################\n",
    "    raw_llm_output, validated_output, *rest = qa_relevance_guard.parse(\n",
    "    llm_output=raw_answer, metadata={'question':question}\n",
    "    )\n",
    "    if validated_output:\n",
    "        raw_llm_output, validated_output, *rest = provenance_v1_guard.parse(\n",
    "            llm_output=raw_answer, \n",
    "            metadata={'query_function':partial(query_function, sources=[i.text for i in source_nodes])}\n",
    "        )\n",
    "        if validated_output:\n",
    "            answer = validated_output\n",
    "            \n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ff6de-3abb-4f84-9a12-a28ea154f426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43e5dae0-a76a-4c93-bd25-25e37d78785f",
   "metadata": {},
   "source": [
    "### Guardrails ai examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9080e3-b9fb-4eec-8365-438571462d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QARelevanceLLMEval \n",
    "qa_relevance_guard = gd.Guard.from_string(\n",
    "    validators=[\n",
    "        QARelevanceLLMEval(on_fail=\"fix\", llm_callable=\"gpt-3.5-turbo\"),\n",
    "    ],\n",
    "    description=\"\",\n",
    ")\n",
    "\n",
    "raw_response = response.response\n",
    "question = questions[0]\n",
    "raw_llm_output, validated_output, *rest = qa_relevance_guard.parse(\n",
    "    llm_output=raw_response, metadata={'question':question}\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(f\"### raw_response ###:\\n{raw_response}\")\n",
    "print(f\"### question ###:\\n{question}\")\n",
    "print(f\"### validated_output ###:\\n{validated_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec84f51-9cc5-4de1-9fd8-8cd6dd9bf2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a454b479-80ef-41e0-9354-a93afd944d7f",
   "metadata": {},
   "source": [
    "### Check similarity with FAQ with Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3fefa-3ff8-4f22-8eac-2b1fa789ada8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check with SimilarToDocument if the question is in the excel list of questions #####\n",
    "excel_files = list_all_filepaths(common_dir=DATA_DIR, folder='',extension='xlsx')\n",
    "faq_df = pd.read_excel(excel_files[0])\n",
    "excel_questions = faq_df.Domanda.values\n",
    "\n",
    "real_question = \"Qual è l'obiettivo principale del Bando Wikipedia\"\n",
    "response = None\n",
    "\n",
    "for excel_q in tqdm(excel_questions[:2]):\n",
    "    guard = gd.Guard.from_string(\n",
    "        validators=[\n",
    "            SimilarToDocument(model='text-embedding-ada-002', document=excel_q, threshold=0.9, on_fail=\"filter\"),\n",
    "        ],\n",
    "        description=\"testmeout\",\n",
    "    )\n",
    "\n",
    "    raw_output, validated_output, reask, validation_passed, error = guard.parse(\n",
    "        llm_output=real_question, num_reasks=1\n",
    "    )\n",
    "    if validated_output:\n",
    "        response = faq_df.loc[faq_df.Domanda==excel_q, \"Risposta\"].values[0]\n",
    "        break\n",
    "\n",
    "# Print the output\n",
    "print(f\"### raw_output ###:\\n{raw_output}\")\n",
    "print(f\"### real_question ###:\\n{real_question}\")\n",
    "print(f\"### response ###:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260b275-d276-4ed7-9222-ba874476b89e",
   "metadata": {},
   "source": [
    "### Query with reranking: Version O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb20317-ea7a-4ca0-8b19-1b6941bd7666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rerank_postprocessor = TimeWeightedPostprocessor(time_decay=0.5, time_access_refresh=False, top_k=1)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=False, \n",
    "    response_mode=\"tree_summarize\",\n",
    "    verbose=True,\n",
    "    similarity_top_k=3,\n",
    "    node_postprocessors=[rerank_postprocessor], \n",
    "    text_qa_template=None,\n",
    ")\n",
    "# Query and Print Response\n",
    "question = excel_questions[0]\n",
    "response = query_engine.query(question)\n",
    "print(f\"QUESTION: {question}\")\n",
    "print(response.response)\n",
    "\n",
    "# len(response.source_nodes)\n",
    "# response.source_nodes[1].node.get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889d319-8f5f-468a-a2cd-e4d0b253fe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8889cc-d081-482c-99ec-fe92241aa7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8588aa1-a8ae-499b-962d-b42e83793d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2b9c9-0def-4e08-b8c4-23914d770d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bed75-95fe-4600-8cf1-2d6ac366ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21035b6a-efa1-47d4-9f8c-1ed7d6173b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deb45f-65aa-474b-bc7e-9762ba01b175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6f4e6-795b-44c9-886b-fc08d2566b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc704e7-fc31-46a6-83ad-8145a87b9247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9826355-0183-4246-868b-0a98de00fc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53c847-142d-445b-9d28-5ea3ce088cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0770a774-34d9-4454-864f-9f3fd42a48cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1ce34-c810-40f6-a717-487be1485503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08661633-538c-464f-b573-00a9a6367fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63087d1-98e0-4dc1-95eb-096e2310910d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aae827-ce38-4eea-91b7-790cf4a34bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c33e5b-76ce-4b6e-b202-00f024e0cf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2829e24-6cf9-49e0-a84f-89635a3fd839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef32fc1-4937-4062-9230-c0522c77c0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3864ab8-9e00-457b-b4c3-1921575cdfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff1eda-df1e-4771-8238-d711b7aae8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24623be9-9ff7-49a8-ae1f-a1922f51b9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3b2fe-ef2c-4fed-9136-7b55cd97a2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3406bc6-1267-49c9-a706-a9a1a9feb56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85eaf77-2145-407a-8a81-3dbd44298f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb1963-a6b2-400b-a8bf-f9872837cfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6761154-8c26-4caa-887a-96815b702a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6bc03-8802-4d2d-b617-fc70389bf785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2df5e9-f0c6-476a-9370-e3f9c9864260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a95ba386-6a06-4b0a-a8e9-0bc897d66aea",
   "metadata": {},
   "source": [
    "# Create validation questions for the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128bacf-bf3f-4fda-9dc9-42a313839182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.evaluation import (\n",
    "    DatasetGenerator, \n",
    "    RelevancyEvaluator, \n",
    "    ResponseEvaluator, \n",
    "    FaithfulnessEvaluator, \n",
    "    QueryResponseEvaluator,\n",
    ")\n",
    "from llama_index import (\n",
    "    Response,\n",
    "    load_index_from_storage,\n",
    "    SummaryIndex\n",
    ")\n",
    "from llama_index.prompts import Prompt\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "# from llama_index.llms import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4643fd-92d4-4794-a44c-1ad9e70be446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=db_collection)\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               embed_model=embedding_model,\n",
    "                                               node_parser=node_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be6107-19d0-43d6-b062-4ce29274d564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = data\n",
    "data_generator = DatasetGenerator.from_documents(\n",
    "                      documents,\n",
    "                      text_question_template=Prompt(\n",
    "                      \"A sample from the documents written in Italian is below.\\n\"\n",
    "                      \"---------------------\\n\"\n",
    "                      \"{context_str}\\n\"\n",
    "                      \"---------------------\\n\"\n",
    "                      \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "                      \"{query_str}\"\n",
    "                      ),\n",
    "                      question_gen_query=(\n",
    "                          \"You are a search pipeline evaluator. Using the papers provided, \"\n",
    "                          \"you must create a list of summary questions in Italian. \"\n",
    "                          \"Limit the queries to the information supplied in the context.\\n\"\n",
    "                          \"Question: \"\n",
    "                      ),\n",
    "                      service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa43dc5-15d1-4902-b6cf-b11acecd2858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To avoid RuntimeError: asyncio.run() cannot be called from a running event loop when running in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc3d88-7189-4e2b-a4c0-8056cb59ad34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated_questions  = data_generator.generate_questions_from_nodes(num=10)\n",
    "print(f\"Generated {len(generated_questions)} questions.\")\n",
    "\n",
    "# save the questions into a txt file for resuse later on\n",
    "out_file_path = os.path.join(OUTPUT_DIR, \"validation_questions.txt\")\n",
    "with open(out_file_path, \"w\") as f:\n",
    "    for question in generated_questions:\n",
    "        f.write(f\"{question.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1694b2-0bab-4719-a332-883ccabd5851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_file_path = os.path.join(OUTPUT_DIR, \"validation_questions.txt\")\n",
    "with open(out_file_path, 'r') as f:\n",
    "    generated_questions = f.readlines()\n",
    "    generated_questions = [line.rstrip() for line in generated_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e7532-6cf7-4a4c-ae28-fc57d61b2643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ee59c-7b9e-4678-82d4-755f51fbc558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009c3df-c550-42a9-9257-209a35ba7073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
